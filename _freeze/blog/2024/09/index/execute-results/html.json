{
  "hash": "dfc97be840848d0e0245844f083390ea",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"bareâ€“bones linear regression\"\ndate: now\ndescription: \"custom lm implementation\"\nformat:\n  html:\n    css: styles.css\n    code-fold: true\n    code-tools: true\n    code-overflow: wrap\n    code-line-numbers: false\n#doi: \ncitation: true\ndraft: true\n---\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n## the plan\n\nImplementing a linear regression algorithm from scratch in R is an excellent way to gain a deeper understanding of both the statistical concepts behind regression and the computational methods used to perform it. The goal is to walk through each step required to build a simple linear regression model without relying on any external packages or built-in functions like `lm`ðŸ˜…. We'll look at the mathematical foundations, step-by-step implementation in R, and ways to evaluate the model.\n\n## how to think about glm\n\n#### a matrix\nTo more easily understand and interpreting regression methods, think of a set of data points with $n$ number of variables as matrices in the following form: \n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](img/matrix.png){fig-align='center' width=90%}\n:::\n:::\n\n\nwhere matrix $X$ contains $n$ columns for $m$ number of variables, and for each point $(x,y)$ we have one corresponding $\\beta$ coefficient and error term $\\epsilon$. \n\n#### a vector in 2d space\n\nAdditionally, you can think of each column in the data as geometric vector, where each variable is represented by a spatial vector. Its length describes its variability^[squared length of the vector is the sum of squares associated with the variable, $|y|^2 = SS_{y}$] and the angle^[$r_{xy} = corr(x_{i},y_{j}) = cos\\angle(x_{i},y_{j})$] between two vectorsâ€“or variablesâ€“ represents the association between them.\n\nA linear combination of two variables is then shown by their vector sum. For example, $x_{1} + 2x_{2}$ is represented by:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-html/vector-sum-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nThe resulting vector will represent the properties of the new variable, its variability and correlation to others. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ndata(mtcars)\n\n# Standardize magnitude for horsepower (hp) and weight (wt)\nmtcars$hp_norm <- mtcars$hp/max(mtcars$hp)\nmtcars$wt_norm <- mtcars$wt/max(mtcars$wt)\n\n# Create a data farme with start and end point vectors\nvectors <- data.frame(\n  x1 = mtcars$hp_norm,\n  y1 = mtcars$wt_norm,\n  car = rownames(mtcars)\n)\n\nvector_plot <- ggplot(vectors) +\n  geom_segment(aes(x = 0,y = 0,\n                   xend = x1, yend = y1),\n  arrow = arrow(length = unit(0.15,\"cm\")),\n  linewidth = .5, color = happy_clrs[6]) +\n  geom_text(aes(x = jitter(x1, amount = 0.1), y = jitter(y1, amount = 0.1), \n                label = car),\n            hjust = 0.1, vjust = .5,\n            size = 6, color = happy_clrs[4],\n            check_overlap = TRUE) +\n  xlim(0,1.5) +\n  ylim(0,1.5) +\n  coord_fixed() +\n  theme_happy() +\n    labs(title = \"Car Horsepower(hp) vs. Weight(wt)\",\n       x = NULL,\n       y = NULL)\n\n# ggsave(\"img/vectors.png\", vector_plot, width = 6, height = 4.5)\n\nknitr::include_graphics(\"img/vectors.png\")\n```\n\n::: {.cell-output-display}\n![](img/vectors.png){fig-align='center' width=90%}\n:::\n:::\n\n\n## the general linear model\n\nThe GLM is a method by which one outcome variable is represented by a combination of variables $X_{n}$. In simple linear regression, the **linear** relationship between the dependent variable $y$ and the independent variable $x$ is modeled as:\n\n$$\ny = \\beta_{0} + \\beta_{1}x + \\epsilon \n$$\n\nwhere:   \n$\\beta_{0}$ represents the $y$-intercept,  \n$\\beta_{1}$ represents the slope of the regression line, and   \n$\\epsilon$ is the error term, representing the difference between the observed and predicted values\n\n### multiple linear regression\n\nIn multiple linear regression, the model extends to accommodate more than two variables.\n\n$$\ny = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + ...\\beta_{n}x_{n} + \\epsilon,\n$$\n\nwhere $n$ is the number of variables.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}