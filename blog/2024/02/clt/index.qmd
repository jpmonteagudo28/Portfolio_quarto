---
title: "What's this CLT, please? A straightforward explanation of the Central Limit Theorem and the magic ($n =30$) sample size requirements"
date: 2024-03-06
description: "Shining a light on the Central Limit Theorem and the mythical sample size requirements for the health and social sciences"
format:
  html:
    css: styles.css
    code-fold: true
    code-tools: true
    code-line-numbers: true
doi: 10.59350/bje88-8r592
citation: true
draft: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align = "center", fig.retina = 3,
                      fig.width = 5.5, fig.height = (5.5 * 0.618),
                      out.width = "90%", collapse = TRUE)
options(digits = 3, width = 300)

set.seed(09)
```

## The Central Limit Theorem

The rudiments of the Central Limit Theorem (CLT) can be traced back to Bernoulli, who famously penned: “even the stupidest person without any prior instruction knows that the more observations are made, the less danger there is of missing the target”, hinting at the notion that, as the number of trials increases, even a simpleton can eventually arrive at the expected outcome.

The theorem derives its name from the fact that we're using a parameter of *central* tendency – typically the mean, median, or mode– to arrive at the *limiting distribution* of independent random variables– which, under certain conditions, will typically be the standard normal distribution.

To put it differently, by sampling a finite population an increasing number of times and using the mean of the $n$ samples as our metric, we will see how our histogram for the population of sampled means will conform to a standard normal curve. By estimating the limiting distribution– the beloved normal distribution– of our independent variables, the theorem helps us answer the question:

> If an outcome $p$ has occurred $p$ times out of $p + q = n$ number of trials, what's the probability of our desired outcome occurring $r$ times in a further $r + q = m$ number of trials?

Let's now talk about the **conditions** we must meet for our distribution to converge or arrive at the standard normal distribution with mean equal to zero and variance equal to 1.

::: column-margin
I'll start referring to a standard normal distribution with mean equal to zero and variance equal to 1 by using the following notation $\mathcal{N}(0,1)$
:::

### Random processes

The variables under study must be the outcome of a random phenomenon for which we know the type of outcomes that can happen but cannot know how or when they will occur. We can know some of the causes of this phenomenon, but their deviations from a supposed pattern are unknown to us.

A classic example of a random variable is a game of cards in which we know what the cards on the deck are but cannot tell which one will come up next– unless we cheat and then the whole thing is not a random phenomenon anymore. In the life and social sciences, we can find random variables everywhere: the height and weight of a population, hemoglobin levels in healthy adults, the number of patients in the ER on any given night, pH levels in different types of topsoil or the scores on the UCLA Loneliness scale. The common thread here is that one *random* observation doesn't provide information on the occurrence of the next one.

### Independence and identical distribution

In the life and social sciences, we'll typically see *conditional independence*, where variables $A$ and $B$ are measurements of the same underlying quantity $C$. We say that $A$ and $B$ are conditionally independent if, once C is known, the value of $B$ doesn't give new information about $A$. Knowing what happens to $B$ after $C$ takes place doesn't increase our chances of correctly predicting the outcome of $A$. In addition, independent variables are also uncorrelated – their covariance ($cov[A,B] = E[A]\cdot E[B]$) and Pearson's $r$ ($\rho_{X,Y} = \frac{cov[A,B]}{\sigma_{A}\sigma_{B}}$​) are equal to zero.

```{r, eval = TRUE, attr.source= '.numberLines'}
#| label: fig_iid
#| fig-cap: "Simulated identically distributed and non-identically distributed variables."
#| lightbox:
#| group: r-graph
#| column: margin

clrs <- c( # Creating custom color palette
  "#FFBE00", #  yellow
  "#B92F0A", #  red
  "#7C225C", #  purple
  "#792A26", #  brown
  "#242424", #  dark gray
  "#394DAA") #  blue

xx <- seq(-5,5,by=.01) # Creating variable to use in dens. curve
## Plot graphics
par(las = 1, mfrow = c(1,3), mai = c(.5, .5, .5, .1))
## Plot graphics
plot(xx, dnorm(xx, -1),type = "b", lwd = 1, xlab = "", ylab = "",
  col = clrs[3], main = "Identically distributed")
lines(xx, dnorm(xx, -1), lty = 2, lwd = 3, pch = 18, col = clrs[1])
legend("topright",legend = c("A", "B"),
  col = c(clrs[3], clrs[1]), lty = 1:2, cex = 0.8)
## Plot graphics
plot(xx,dnorm(xx,-1),type="l",lwd=2,xlab="",ylab="",
     col = clrs[3],main="Shift in means")
lines(xx,dnorm(xx,1),lty = 2, lwd=3,pch = 18,col= clrs[1])
legend("topright", legend = c("A", "B"),
       col = c(clrs[3], clrs[1]), lty = 1:2, cex = 0.8)
## Plot graphics
plot(xx,dnorm(xx,-1),type="l",lwd=2,xlab="",ylab="",
     col = clrs[3], main = "Different distributions", ylim = c(0, 0.6))
lines(xx, dchisq(xx, 2), lwd = 2, col = clrs[1])
legend("topright", legend = c("A", "B"),
       col = c(clrs[3], clrs[1]), lty = 1:2, cex = 0.8)
```

Additionally, two variables $A_{n}$ and $B_{n}$ are identically distributed when their probability distributions match exactly, and their means and variances ($\mu_{A} = \mu_{B}, \sigma_{A}^2 = \sigma_{B}^2$​​​) are equal. By now you can already see how this assumption can at best be approximated and almost impossible to demonstrate by the variables commonly studied in the life and social sciences.

Let's look at a hypothetical example of conditional independence:

Let's say a microchip manufacturer records the weekly number of defective microchips (`def_micros`) for each of its 8–hour shifts to evaluate whether the number of complaints (`complaints`) received in 24 hours is related to `def_micro`. They also want to know if the number of complaints given `def_micro` is related to the number of devices (`devices`) bought and returned through their online retailer. . To answer this question, we first need to determine that `complaints` and `devices` are conditionally independent if, each variable is associated with`def_micr0`, but their [partial correlation](https://en.wikipedia.org/wiki/Partial_correlation) coefficient is close to zero. We then calculate the zero-order correlation and compare it to the first–order correlation given the number of defective microchips. In our example, the zero-order correlation between devices and complaints is $\rho = .099$ and the first–order correlation is technically non-existent $\rho = -.021$, and we can assume `devices` and `complaints` are conditionally independent.

```{r, eval = TRUE, warning = FALSE, message = FALSE, attr.source= '.numberLines'}
#| fig-cap-label: cond_ind
#| fig-cap: "Relationship between devices, complaints, and defective microchips"
#| lightbox:
#| group: r-graph
#| column: margin

##---------------------------------------------------------------------------------- ##
## Generating two random variables that meet the i.i.d assumption
## --------------------------------------------------------------------------------- ##
library(here)
library(ppcor)
library(ggplot2)
library(gridExtra)
library(showtext)
library(knitr)
##------------------------------------------------------------------------------------- ##
## Generating two conditionally independent random variables
## ------------------------------------------------------------------------------------ ##
## Creating function to generate random variable with defined correlation to an existing var.
complement <- function(y, rho, x) { # corr. coefficient
  if (missing(x)) x <- rgamma(length(y), 2, .5) # Optional: supply a default if `x` is not given
  y.perp <- residuals(lm(x ~ y))
  rho * sd(y.perp) * y + y.perp * sd(y) * sqrt(1 - rho^2)
}
def_micro <- rpois(1050, 6) # lambda = n*p, p = .006 defective microchips 
complaints <- complement(def_micro, rho = .273) # no. complaints received on 24 hr period 
devices <- complement(def_micro, rho = .331) # weekly no. devices processed at online retailer

## Creating function to summarize list of variables
group_summary <- function(...) {
  summaries <- lapply(list(...), summary)
  names(summaries) <- paste0("Summary_", seq_along(summaries))
  return(summaries)
}

group_summary(def_micro, devices, complaints)
## ------------------------------------------------------------------------------- ##
## Creating function to find Spearman's rho among all variables
## ------------------------------------------------------------------------------- ##
spearman_correlation <- function(...) {
  df <- data.frame(...)
  correlation_matrix <- round(cor(df, method = "spearman"), 3)
  return(correlation_matrix)
}
spearman <- spearman_correlation(def_micro, devices, complaints)
##
##              def_micro devices complaints
## def_micro      1.000   0.378      0.297
## devices        0.378   1.000      0.099
## complaints     0.297   0.099      1.000


## Use ppcor package to calculate partial correlation between complaints and devices
part_cor <- ppcor::pcor.test(devices, complaints, def_micro, method = "spearman")
##      estimate  p.value  statistic  n gp   Method
##  -0.02075897 0.501827 -0.6718506 1050  1 spearman
## Relationship between devices and complaints is technically non-existent.
## We can say they're conditionally independent.
## -------------------------------------------------------------------------------- ##
## Creating scatterplots to visualize relationship between variables
## -------------------------------------------------------------------------------- ##
# Custom ggplot theme to make nicer plots
# Get the font at https://fonts.google.com/specimen/Fira+Sans+Condensed
font_add_google("Fira Sans")
showtext_auto()
theme_nice <- function() {
  theme_minimal(base_family = "Fira Sans Condensed") +
    theme(
      panel.grid.minor = element_blank(),
      plot.title = element_text(family = "Fira Sans Condensed Bold", face = "plain", size = rel(1.35)),
      plot.subtitle = element_text(family = "Fira Sans Condensed Medium", face = "plain", size = rel(1.2)),
      axis.title = element_text(family = "Fira Sans Condensed SemiBold", face = "plain", size = rel(1)),
      axis.title.x = element_text(hjust = 0),
      axis.title.y = element_text(hjust = 1),
      axis.text = element_text(family = "Fira Sans Condensed Light", face = "plain", size = rel(0.8)),
      strip.text = element_text(
        family = "Fira Sans Condensed", face = "bold",
        size = rel(1), hjust = 0
      ),
      strip.background = element_rect(fill = "grey95", color = NA)
    )
}

theme_nice_dist <- function() {
  theme_nice() +
    theme(
      panel.grid = element_blank(),
      panel.spacing.x = unit(10, units = "pt"),
      axis.ticks.x = element_line(linewidth = 0.25),
      axis.text.y = element_blank()
    )
}

theme_set(theme_nice())

ggplot2::update_geom_defaults("label", list(family = "Fira Sans Condensed SemiBold", fontface = "plain"))
ggplot2::update_geom_defaults("text", list(family = "Fira Sans Condensed SemiBold", fontface = "plain"))

# Making df with previously created variables
df1 <- data.frame(def_micro, devices, complaints)
# Making plot to show correlation between devices v. complaints.
dev_comp_plot <- ggplot2::ggplot(df1, aes(devices, complaints)) +
  geom_point(color = clrs[2]) +
  geom_smooth(method = "glm", # Use either glm or gam models
    se = FALSE,
    color = clrs[5],
    linewidth = 1) +
  labs(
    x = "devices",
    y = "complaints") +
  theme_nice_dist() # Apply a custom theme
# Making plot to show correlation between devices v. microchips
dev_micro_plot <- ggplot2::ggplot(df1, aes(devices, def_micro)) +
  geom_point(color = clrs[3]) +
  geom_smooth(method = "lm",
    se = FALSE,
    color = clrs[5],
    linewidth = 1) +
  labs(
    x = "devices",
    y = "microchips") +
  theme_nice_dist() # Apply a custom theme
# Making plot to show correlation between complaints v. microchips
comp_micro_plot <- ggplot2::ggplot(df1, aes(complaints, def_micro)) +
  geom_point(color = clrs[6]) +
  geom_smooth(method = "lm",
    se = FALSE,
    color = clrs[5],
    linewidth = 1) +
  labs(
    x = "complaints",
    y = "microchips") +
  theme_nice_dist() # Apply a custom theme
# Creating grid panel displaying all three plots vertically. 
# I prefer the vertical plots rather than the horizontal ones
arranged_plots <- gridExtra::grid.arrange(dev_comp_plot, dev_micro_plot, comp_micro_plot, nrow = 1) 

```

### Additive processes

A process $Y$ is considered additive when the value of each observation is determined by adding a variable quantity to the value of a previous observation. The process is characterized by an initial value $Y(0)$ and the increments $Y_{i} - Y_{h}$ with $i > h \geq 0$, where $Y_{i}$ represents the value of an observation at a certain point and $Y_{h}$, the value of the subsequent observation. This additive process typically consists of three components.

-   An initial value $Y_{0}$ that serves as the starting point of the process.

-   A random, slow, and gradual change component $Y_{1}$ around the central tendency represents the continuous evolution of the process over time.

-   A third random component $Y_{2}$​ represents sudden jumps or changes in the process, which may occur unpredictably at certain time points.

Together, these components contribute to the behavior of the $Y$​ process and its evolution across time through a combination of unexpected slow changes and sudden jumps.

An example of such a random process in epidemiology is the *total number* of infections within a population over time. Initially, there's a small number of infected (*I*) individuals that spread the disease across the susceptible population (*S*), and as the population grows and control measures take place, the evolution of the disease and the number of cases fluctuates. If a new variant of this disease was introduced, a sudden spike in the *total number* of infected individuals would also determine the evolution of the disease.

#### Are processes always additive?

Unfortunately for us, not all processes are additive. Multiplicative processes are common in nature and sometimes we mistakenly assume additivity. Let's clarify the difference between an additive and a multiplicative process.

A random process is considered multiplicative when the value of a variable $Y_{t}$ at each time step *multiplicatively* depends on the previous value $Y_{t-1}$ by a scaling factor $r$. To get to the next observation, $Y_{t}$, we have to multiply by $r$ instead of adding this random quantity to our initial value $Y_{0}$. [$Y_{t} = Y_{t-1} \cdot r \,, \, r = \nu(1 + \rho_{t})$, where $\nu \geq 0$ and the random component $\rho_{t} \geq 0$]{.aside} The scaling factor $r$​ changes with each observation as random variation is introduced.

-   How we use CLT

-   How to know when empirical distribution converges to normal distribution?

### What's the deal with $n \geq 30$?

Jacob Cohen (1991) mentioned that at least 30 observations were needed to use the critical-ratio approach used in t-tables when comparing groups. If the sample size is smaller than 30, then students would be forced to “small” sample statistics.

> It wasn't until some years later that I discovered (mind you, not invented) power analysis, one of whose fruits was the revelation that for a two-independent-group-mean comparison with n = 30 per group at the sanctified two-tailed .05 level, the probability that a medium-sized effect would be labeled as significant by the most modern methods (a t-test) was only .47. Thus, it was approximately a coin flip whether one would get a significant result, even though, in reality, the effect size was meaningful.

Simply a rule of thumb that allowed students to use critical t–t-tables. These tables could only span 30 lines to fit in one page. Fisher created the t-table and only went up to n = 30 (Fisher's *Statistical Methods for Research Workers* (1925))

Sample size at least 30 so that the error variance between t–distribution and theoretical normal distribution is .25 or less from df = 30 up to infinity.

When data is severely skewed neither the t–test nor the permutation test have much power (are not robust) to correctly identify a statistically significance difference in means between too highly skewed distributions. Even if your data contained tens of thousands of observations, the t–test may not recognize a statistically significance difference in means. The distribution of the data and the number of observations need to be considered when deciding whether the t–test is meaningful and accurate, sometimes thousands of observations may be needed.

## Solution

+ The number of observations needed for our distribution to approximate the standard normal curve will ultimately depend on our data, the question we're seeking to answer, and the test we plan to use to arrive at our hypothesis.
+ Use power analysis to determine the sample size needed to obtain statistically significant results using your desired $\alpha$, $\beta$, and effect size measure. This way, you don't have to assume
