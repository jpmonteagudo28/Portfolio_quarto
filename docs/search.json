[
  {
    "objectID": "uses/index.html",
    "href": "uses/index.html",
    "title": "What I use",
    "section": "",
    "text": "I’ve gradually discovered a research workflow that suits my needs (and my budget😅) and maximizes productivity.\nI’m leaving a list of the programs/software I regularly use as a way to track the changes to my workflow and also provide guidance to those looking for free resources to increase their efficiency and organization."
  },
  {
    "objectID": "uses/index.html#discovery-and-idea-generation",
    "href": "uses/index.html#discovery-and-idea-generation",
    "title": "What I use",
    "section": "Discovery and Idea Generation",
    "text": "Discovery and Idea Generation\nWriting\n\nI use Joplin– an open-source note-taking app with a markdown editor and customizable plugins. Another great option is Obsidian which includes knowledge graphs to organize your thoughts.\nI use Typora to create stand-alone markdown files. Typora also supports pandoc-flavored markdown which makes it easier to transform markdown to other formats like .docx, .pdf, etc.\nI use a good ol’ Mead Five Star Spiral Fat Lil’ Pocket Notebook for note-taking (books I read, ideas, summaries, schedules, to-do lists). I’ve tried planners, calendars, apps and none of those things seem to keep me on track as brilliantly as a handwritten note in one of those fat little notebooks.\nI use Mendeley as my bibliography and citation manager. The free plan includes 2GB of storage, which is more than I need. I store my bibliographies in Bibtex format.\nI use Overleaf and Detexify which makes finding LateX symbols a breeze. Just draw the symbol you’re looking for and the site provides a list of possible matches with their LateX syntax.\nI use Natural Speech Reader to help with text editing."
  },
  {
    "objectID": "uses/index.html#data-collection-and-analysis",
    "href": "uses/index.html#data-collection-and-analysis",
    "title": "What I use",
    "section": "Data Collection and Analysis",
    "text": "Data Collection and Analysis\nScience and Research\n\nI currently use R and RStudio for my data analysis and graphing needs, and VSCode for everything else.\nI used G-Power and piface for sample size calculations, but their utility decreased as I began to use more complicated models.\nMy main statistical programming software was SPSS until 2020, but its lackluster versatility left me disappointed. I especially didn’t like all the clicking and the way their syntax, output, and data panels were set up. Unfortunately, that’s the software of choice in many universities in the U.S., including mine.\nI also use Notepad ++, a text editor that supports more than two dozen programming languages (not markdown). I sometimes use it alongside VSCode.\nI use Github and Git Bash to store almost everything I write and for version control."
  },
  {
    "objectID": "uses/index.html#publication",
    "href": "uses/index.html#publication",
    "title": "What I use",
    "section": "Publication",
    "text": "Publication\nGraphic Design\n\nI’ve used Canva for every design in this website and personal documents. I also have free access to Adobe InDesign but haven’t used it much.\nI totally recommend Practical Typography, by Matthew Butterick– a typograhpy primer for the graphic designer and the layperson trying to create beautiful documents.\nI use PowerPoint to create simple and stunning presentations. I have free access to Microsoft products, and it’s also the best slide editor I’ve seen.\nI use Google Fonts API for font selection. After reading Matthew Butterick’s typography book, I’m careful about text composition and formatting. My preferred fonts are Arsenal, ETBembo, Roboto Serif, Charter, and Cooper Hewitt.\nProductivity\n\nI use Dropbox and Google One for file storage and backup. I get Dropbox for free through my school, and I pay for a family Google One subscription. However, both services cost about the same and offer similar storage space (3TB and 2TB respectively).\nI use the Windows Clock app focus feature to track my time. I’m not easily distracted if I’m working towards a concrete, timed goal so scheduling times of intense mental activity is a huge help.\nI use Otter.ai as my audio recording and automatic transcription service. I record presentations, interviews, talks, etc. and Otter.ai produces a solid transcript that I can then save or use in content creation.\nI use Chat GPT to answer all sorts of questions and to guide me in the creative process."
  },
  {
    "objectID": "uses/index.html#hardware",
    "href": "uses/index.html#hardware",
    "title": "What I use",
    "section": "Hardware",
    "text": "Hardware\n\nI use Yubico and Keybase for privacy and security.\nI use a 2020 15.6″ i5Core Dell, a 2020 13″ MacBook Pro, and an iPhone 13 mini."
  },
  {
    "objectID": "uses/index.html#guides",
    "href": "uses/index.html#guides",
    "title": "What I use",
    "section": "Guides",
    "text": "Guides\n\n\nGood enough practices in scientific computing detail the process of organizing, structuring, and sharing data and research with collaborators while keeping track of all the changes.\n\nFour steps to an applied micro paper, by Jesse Shapiro, outlines the process of writing an academic paper in the applied sciences. It’s a very succinct guide to help students and researchers improve their writing.\n.How to give an applied micro talk, also by Jesse Shapiro, is a brief explanation of why your presentations should be short and engaging, not so tediously structured and technical.\n\nPublic speaking for academic economists, by Rachel Meager, provides a simple and witty guide to public speaking for researchers and academics.\n\nThe Plain Person’s Guide to Plain Text Social Science. A useful primer on organizing and structuring your writing and research process in the Social Science sphere. It provides a template for research and writing that you can transform to suit your needs and budget (I mostly use free software except for Typora and Google One).\nKieran Healy’s Making Slides guide to creating engaging slides by using layers, highlighting, and repetition to build your argument."
  },
  {
    "objectID": "research/articles/index.html",
    "href": "research/articles/index.html",
    "title": "On the Surgical Removal of Cardassian Cranial Implants: A case study of a Cardassian patient",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/articles/index.html#important-links",
    "href": "research/articles/index.html#important-links",
    "title": "On the Surgical Removal of Cardassian Cranial Implants: A case study of a Cardassian patient",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/articles/index.html#abstract",
    "href": "research/articles/index.html#abstract",
    "title": "On the Surgical Removal of Cardassian Cranial Implants: A case study of a Cardassian patient",
    "section": "Abstract",
    "text": "Abstract\nI have reset the sensors to scan for frequencies outside the usual range. By emitting harmonic vibrations to shatter the lattices. We will monitor and adjust the frequency of the resonators. He has this ability of instantly interpreting and extrapolating any verbal communication he hears. It may be due to the envelope over the structure, causing hydrogen-carbon helix patterns throughout. I’m comparing the molecular integrity of that bubble against our phasers."
  },
  {
    "objectID": "research/articles/index.html#important-notes",
    "href": "research/articles/index.html#important-notes",
    "title": "On the Surgical Removal of Cardassian Cranial Implants: A case study of a Cardassian patient",
    "section": "Important notes",
    "text": "Important notes\nStellar flares are increasing in magnitude and frequency. Set course for Rhomboid Dronegar 006, warp seven. There’s no evidence of an advanced communication network. Total guidance system failure, with less than 24 hours’ reserve power. Shield effectiveness has been reduced 12 percent. We have covered the area in a spherical pattern which a ship without warp drive could cross in the given time.\nResistance is futile."
  },
  {
    "objectID": "research/articles/index.html#citation",
    "href": "research/articles/index.html#citation",
    "title": "On the Surgical Removal of Cardassian Cranial Implants: A case study of a Cardassian patient",
    "section": "Citation",
    "text": "Citation\n\n Add to Mendeley \n\n@article{BashirGarak:2178,\n    Author = {Julian Bashier and Elim Garak},\n    Journal = {Starbase Deep Space Nine},\n    Month = {9},\n    Number = {9},\n    Pages = {1425--1439},\n    Title = {On the Surgical Removal of Cardassian Cranial Implants: A case study of a Cardassian patient},\n    Volume = {130},\n    Year = {2178}}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "¡Bienvenido!",
    "section": "",
    "text": "I’m an aspiring epidemiologist currently pursuing post-graduate studies in Epidemiology at Liberty University. I have previously obtained degrees in psychology and biostatistics in 2020 and 2023.\nI’m interested in clinical epidemiology, epidemiological research methods, infectious diseases, pharmaco-epidemiology and nutritional epidemiology. I absolutely love mathematical applied statistics and the R programming language, one of the best tools for clinical statistics and epidemiological research.\nI’m currently taking a deep dive into generalized linear models, causal inference and data visualization. Nowadays, I’m conducting a pilot study to analyze the impact of new hire training on performance in a fast-paced contact center environment for the financial services division of a large university."
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "Blog",
    "section": "2024",
    "text": "2024\n\n\n    \n    \n                  \n            March 17, 2024\n        \n        \n            Don't be an absolutist. Use the `here` package for reproducible workflows\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    packages\n                \n                \n                \n                    here\n                \n                \n                \n                    working dir\n                \n                \n                \n                    setwd\n                \n                \n                \n                    programming\n                \n                \n                \n                    code errors\n                \n                \n                \n                    workflow\n                \n                \n                \n                    reproducibility\n                \n                \n            \n            \n\n            Gentle reminders when using the `here` package for your `RStudio` projects\n            \n            \n            10.59350/4a9fr-acc34\n            \n        \n        \n    \n    \n    \n                  \n            March 6, 2024\n        \n        \n            What's this CLT, please? A straightforward explanation of the Central Limit Theorem and the magic ($n =30$) sample size requirements\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    clt\n                \n                \n                \n                    sample size\n                \n                \n                \n                    t–test\n                \n                \n                \n                    normal\n                \n                \n                \n                    Lyndeberg-Lévy\n                \n                \n                \n                    i.i.d\n                \n                \n                \n                    Gaussian\n                \n                \n                \n                    random\n                \n                \n            \n            \n\n            Shining a light on the Central Limit Theorem and the mythical sample size requirements for the health and social sciences\n            \n            \n            10.59350/bje88-8r592\n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/2024/02/clt/index.html",
    "href": "blog/2024/02/clt/index.html",
    "title": "What’s this CLT, please? A straightforward explanation of the Central Limit Theorem and the magic (\\(n =30\\)) sample size requirements",
    "section": "",
    "text": "The rudiments of the Central Limit Theorem (CLT) can be traced back to Bernoulli, who famously penned: “even the stupidest person without any prior instruction knows that the more observations are made, the less danger there is of missing the target”, hinting at the notion that, as the number of trials increases, even a simpleton can eventually arrive at the expected outcome.\nThe theorem derives its name from the fact that we’re using a parameter of central tendency – typically the mean, median, or mode– to arrive at the limiting distribution of independent random variables– which, under certain conditions, will typically be the standard normal distribution.\nTo put it differently, by sampling a finite population an increasing number of times and using the mean of the \\(n\\) samples as our metric, we will see how our histogram for the population of sampled means will conform to a standard normal curve. By estimating the limiting distribution– the beloved normal distribution– of our independent variables, the theorem helps us answer the question:\n\nIf an outcome \\(p\\) has occurred \\(p\\) times out of \\(p + q = n\\) number of trials, what’s the probability of our desired outcome occurring \\(r\\) times in a further \\(r + q = m\\) number of trials?\n\nLet’s now talk about the conditions we must meet for our distribution to converge or arrive at the standard normal distribution with mean equal to zero and variance equal to 1.\n\n\nI’ll start referring to a standard normal distribution with mean equal to zero and variance equal to 1 by using the following notation \\(\\mathcal{N}(0,1)\\)\n\nThe variables under study must be the outcome of a random phenomenon for which we know the type of outcomes that can happen but cannot know how or when they will occur. We can know some of the causes of this phenomenon, but their deviations from a supposed pattern are unknown to us.\nA classic example of a random variable is a game of cards in which we know what the cards on the deck are but cannot tell which one will come up next– unless we cheat and then the whole thing is not a random phenomenon anymore. In the life and social sciences, we can find random variables everywhere: the height and weight of a population, hemoglobin levels in healthy adults, the number of patients in the ER on any given night, pH levels in different types of topsoil or the scores on the UCLA Loneliness scale. The common thread here is that one random observation doesn’t provide information on the occurrence of the next one.\n\nIn the life and social sciences, we’ll typically see conditional independence, where variables \\(A\\) and \\(B\\) are measurements of the same underlying quantity \\(C\\). We say that \\(A\\) and \\(B\\) are conditionally independent if, once C is known, the value of \\(B\\) doesn’t give new information about \\(A\\). Knowing what happens to \\(B\\) after \\(C\\) takes place doesn’t increase our chances of correctly predicting the outcome of \\(A\\). In addition, independent variables are also uncorrelated – their covariance (\\(cov[A,B] = E[A]\\cdot E[B]\\)) and Pearson’s \\(r\\) (\\(\\rho_{X,Y} = \\frac{cov[A,B]}{\\sigma_{A}\\sigma_{B}}\\)​) are equal to zero.\n\nCodeclrs &lt;- c( # Creating custom color palette\n  \"#FFBE00\", #  yellow\n  \"#B92F0A\", #  red\n  \"#7C225C\", #  purple\n  \"#792A26\", #  brown\n  \"#242424\", #  dark gray\n  \"#394DAA\") #  blue\n\nxx &lt;- seq(-5,5,by=.01) # Creating variable to use in dens. curve\n## Plot graphics\npar(las = 1, mfrow = c(1,3), mai = c(.5, .5, .5, .1))\n## Plot graphics\nplot(xx, dnorm(xx, -1),type = \"b\", lwd = 1, xlab = \"\", ylab = \"\",\n  col = clrs[3], main = \"Identically distributed\")\nlines(xx, dnorm(xx, -1), lty = 2, lwd = 3, pch = 18, col = clrs[1])\nlegend(\"topright\",legend = c(\"A\", \"B\"),\n  col = c(clrs[3], clrs[1]), lty = 1:2, cex = 0.8)\n## Plot graphics\nplot(xx,dnorm(xx,-1),type=\"l\",lwd=2,xlab=\"\",ylab=\"\",\n     col = clrs[3],main=\"Shift in means\")\nlines(xx,dnorm(xx,1),lty = 2, lwd=3,pch = 18,col= clrs[1])\nlegend(\"topright\", legend = c(\"A\", \"B\"),\n       col = c(clrs[3], clrs[1]), lty = 1:2, cex = 0.8)\n## Plot graphics\nplot(xx,dnorm(xx,-1),type=\"l\",lwd=2,xlab=\"\",ylab=\"\",\n     col = clrs[3], main = \"Different distributions\", ylim = c(0, 0.6))\nlines(xx, dchisq(xx, 2), lwd = 2, col = clrs[1])\nlegend(\"topright\", legend = c(\"A\", \"B\"),\n       col = c(clrs[3], clrs[1]), lty = 1:2, cex = 0.8)\n\n\n\n\nSimulated identically distributed and non-identically distributed variables.\n\n\nTwo variables \\(A_{n}\\) and \\(B_{n}\\) are identically distributed when their probability distributions match exactly, and their means and variances (\\(\\mu_{A} = \\mu_{B}, \\sigma_{A}^2 = \\sigma_{B}^2\\)​​​) are equal. By now you can already see how this assumption can at best be approximated and almost impossible to demonstrate by the variables commonly studied in the life and social sciences.\nLet’s look at a scenario in R:\nLet’s say a microchip manufacturer records the weekly number of defective microchips (def_micros) for each of its 8–hour shifts to evaluate whether the number of complaints (complaints) received in a 24 hours has to do with def_micro. They also want to know if the number of complaints given def_micro is related to the number of devices (devices) bought and returned through their online retailer. We would say that the complaints and devices are conditionally independent if, each variable is clearly associated withdef_micr0, but their partial correlation coefficient is close to zero (\\(\\rho = -.021\\)).\n\nCode##---------------------------------------------------------------------------------##\n## Generating two random variables that meet the i.i.d assumption\n## --------------------------------------------------------------------------------- ##\nlibrary(here)\nlibrary(ppcor)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(showtext)\nlibrary(knitr)\n##------------------------------------------------------------------------------------- \n## Generating two conditionally independent random variables\n## ------------------------------------------------------------------------------------ ##\n## Creating function to generate random variable with defined correlation to an existing var.\ncomplement &lt;- function(y, rho, x) { # corr. coefficient\n  if (missing(x)) x &lt;- rgamma(length(y), 2, .5) # Optional: supply a default if `x` is not given\n  y.perp &lt;- residuals(lm(x ~ y))\n  rho * sd(y.perp) * y + y.perp * sd(y) * sqrt(1 - rho^2)\n}\ndef_micro &lt;- rpois(1050, 6) # lambda = n*p, p = .006 defective microchips manufactured by company in 12 hrs\ncomplaints &lt;- complement(def_micro, rho = .273) # no. complaints received on 24 hr period for devices using the microchip in question\ndevices &lt;- complement(def_micro, rho = .331) # weekly no. devices processed at online retailer (+) bought (-)returned\n##\n## Creating function to summarize list of variables\ngroup_summary &lt;- function(...) {\n  summaries &lt;- lapply(list(...), summary)\n  names(summaries) &lt;- paste0(\"Summary_\", seq_along(summaries))\n  return(summaries)\n}\n\ngroup_summary(def_micro, devices, complaints)\n## $Summary_1\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    0.00    4.00    6.00    6.09    8.00   14.00 \n## \n## $Summary_2\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    -7.2     0.8     4.6     6.0     9.8    48.9 \n## \n## $Summary_3\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    -7.2    -0.3     3.3     4.5     8.0    34.8\n\n## ------------------------------------------------------------------------------- ##\n## Creating function to find Spearman's rho among all variables\n## ------------------------------------------------------------------------------- ##\nspearman_correlation &lt;- function(...) {\n  df &lt;- data.frame(...)\n  correlation_matrix &lt;- round(cor(df, method = \"spearman\"), 3)\n  return(correlation_matrix)\n}\n\nspearman &lt;- spearman_correlation(def_micro, devices, complaints)\n##\n##              def_micro devices complaints\n## def_micro      1.000   0.378      0.297\n## devices        0.378   1.000      0.099\n## complaints     0.297   0.099      1.000\n##\n## Use ppcor package to calculate partial correlation between complaints and devices\n\npart_cor &lt;- ppcor::pcor.test(devices, complaints, def_micro, method = \"spearman\")\n##      estimate  p.value  statistic  n gp   Method\n##  -0.02075897 0.501827 -0.6718506 1050  1 spearman\n## Relationship between devices and complaints is technically non-existent. We can say they're conditionally independent.\n## -------------------------------------------------------------------------------- ##\n## Creating scatterplots to visualize relationship between variables\n## -------------------------------------------------------------------------------- ##\n# Custom ggplot theme to make nicer plots\n# Get the font at https://fonts.google.com/specimen/Fira+Sans+Condensed\nfont_add_google(\"Fira Sans\")\nshowtext_auto()\ntheme_nice &lt;- function() {\n  theme_minimal(base_family = \"Fira Sans\") +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(\n        family = \"Fira Sans Bold\",\n        face = \"plain\",\n        size = rel(1.35)\n      ),\n      plot.subtitle = element_text(\n        family = \"Fira Sans Medium\",\n        face = \"plain\",\n        size = rel(1.2)\n      ),\n      axis.title = element_text(\n        family = \"Fira Sans SemiBold\",\n        face = \"plain\",\n        size = rel(1)\n      ),\n      axis.title.x = element_text(hjust = 0.5),\n      axis.title.y = element_text(hjust = 0.5),\n      axis.text = element_text(\n        family = \"Fira Sans Light\",\n        face = \"plain\",\n        size = rel(0.8)\n      ),\n      strip.text = element_text(\n        family = \"Fira Sans\",\n        face = \"bold\",\n        size = rel(1),\n        hjust = 0\n      ),\n      strip.background = element_rect(fill = \"grey95\", color = NA)\n    )\n}\n\ntheme_nice_dist &lt;- function() {\n  theme_nice() +\n    theme(\n      panel.grid = element_blank(),\n      panel.spacing.x = unit(10, units = \"pt\"),\n      axis.ticks.x = element_line(linewidth = 0.25),\n      axis.text.y = element_blank()\n    )\n}\n\ntheme_set(theme_nice())\n# Updating defaults to chosen font\nggplot2::update_geom_defaults(\"label\",list(family = \"Fira Sans SemiBold\", fontface = \"plain\"))\nggplot2::update_geom_defaults(\"text\",list(family = \"Fira Sans\", fontface = \"plain\"))\n\n# Making df with previously created variables\ndf1 &lt;- data.frame(def_micro, devices, complaints)\n# Making plot to show correlation between devices v. complaints.\ndev_comp_plot &lt;- ggplot2::ggplot(df1, aes(devices, complaints)) +\n  geom_point(color = clrs[2]) +\n  geom_smooth(method = \"glm\", # Use either glm or gam models\n    se = FALSE,\n    color = clrs[5],\n    linewidth = 1) +\n  labs(\n    x = \"devices\",\n    y = \"complaints\") +\n  theme_nice_dist() # Apply a custom theme\n# Making plot to show correlation between devices v. microchips\ndev_micro_plot &lt;- ggplot2::ggplot(df1, aes(devices, def_micro)) +\n  geom_point(color = clrs[3]) +\n  geom_smooth(method = \"lm\",\n    se = FALSE,\n    color = clrs[5],\n    linewidth = 1) +\n  labs(\n    x = \"devices\",\n    y = \"microchips\") +\n  theme_nice_dist() # Apply a classic theme\n# Making plot to show correlation between complaints v. microchips\ncomp_micro_plot &lt;- ggplot2::ggplot(df1, aes(complaints, def_micro)) +\n  geom_point(color = clrs[6]) +\n  geom_smooth(method = \"lm\",\n    se = FALSE,\n    color = clrs[5],\n    linewidth = 1) +\n  labs(\n    x = \"complaints\",\n    y = \"microchips\") +\n  theme_nice_dist() # Apply a custom theme\n# Creating grid panel displaying all three plots vertically. \narranged_plots &lt;- gridExtra::grid.arrange(dev_comp_plot, dev_micro_plot, comp_micro_plot, nrow = 1) # I prefer the vertical plots rather than the horizontal ones\n\n\n\n\nRelationship between devices, complaints, and defective microchips\n\n\n\nA process \\(Y\\) is considered additive when the value of each observation is determined by adding a variable quantity to the value of a previous observation. The process is characterized by an initial value \\(Y(0)\\) and the increments \\(Y_{i} - Y_{h}\\) with \\(i &gt; h \\geq 0\\), where \\(Y_{i}\\) represents the value of an observation at a certain point and \\(Y_{h}\\), the value of the subsequent observation. This additive process typically consists of three components.\n\nAn initial value \\(Y_{0}\\) that serves as the starting point of the process.\nA random, slow, and gradual change component \\(Y_{1}\\) around the central tendency represents the continuous evolution of the process over time.\nA third random component \\(Y_{2}\\)​ represents sudden jumps or changes in the process, which may occur unpredictably at certain time points.\n\nTogether, these components contribute to the behavior of the \\(Y\\)​ process and its evolution across time through a combination of unexpected slow changes and sudden jumps.\nAn example of such a random process in epidemiology is the spread of an infectious disease within a population over time. Initially, there’s a small number of infected (I) individuals that spread the disease across the susceptible population (S), and as the population grows and control measures take place, the evolution of the disease fluctuates. If a new variant of this disease was introduced, a sudden spike in the number of infected individuals would also determine the evolution of the disease.\n\nHow we use CLT\n\nHow to know when empirical distribution converges to normal distribution?\n\n\n\nJacob Cohen (1991) mentioned that at least 30 observations were needed to use the critical-ratio approach used in t-tables when comparing groups. If the sample size is smaller than 30, then students would be forced to “small” sample statistics.\n\nIt wasn’t until some years later that I discovered (mind you, not invented) power analysis, one of whose fruits was the revelation that for a two-independent-group-mean comparison with n = 30 per group at the sanctified two-tailed .05 level, the probability that a medium-sized effect would be labeled as significant by the most modern methods (a t-test) was only .47. Thus, it was approximately a coin flip whether one would get a significant result, even though, in reality, the effect size was meaningful.\n\n\n\nSimply a rule of thumb that allowed students to use critical t–t-tables. These tables could only span 30 lines to fit in one page. Fisher created the\nt-table and only went up to n = 30 (Fisher’s Statistical Methods for Research Workers (1925))\n\nSample size at least 30 so that the error variance between t–t-distribution and theoretical normal distribution is .25 or less from df = 30 up to infinity.\nWhen data is severely skewed neither the t–test nor the permutation test have much power (are not robust) to correctly identify a statistically significance difference in means between too highly skewed distributions. Even if your data contained tens of thousands of observations, the t–test may not recognize a statistically significance difference in means. The distribution of the data and the number of observations need to be considered when deciding whether the t–test is meaningful and accurate, sometimes thousands of observations may be needed.\n\n\n\n\n\nThe number of observations needed for our distribution to approximate the standard normal curve will ultimately depend on our data, the question we’re seeking to answer, and the test we plan to use to arrive at our hypothesis.\nUse power analysis to determine the sample size needed to obtain statistically significant results using your desired \\(\\alpha\\), \\(\\beta\\), and effect size measure. This way, you don’t have to assume\n\n\n\n\n\n\n\n\nSimulated identically distributed and non-identically distributed variables.\nRelationship between devices, complaints, and defective microchips"
  },
  {
    "objectID": "blog/2024/02/clt/index.html#the-central-limit-theorem",
    "href": "blog/2024/02/clt/index.html#the-central-limit-theorem",
    "title": "What’s this CLT, please? A straightforward explanation of the Central Limit Theorem and the magic (\\(n =30\\)) sample size requirements",
    "section": "",
    "text": "The rudiments of the Central Limit Theorem (CLT) can be traced back to Bernoulli, who famously penned: “even the stupidest person without any prior instruction knows that the more observations are made, the less danger there is of missing the target”, hinting at the notion that, as the number of trials increases, even a simpleton can eventually arrive at the expected outcome.\nThe theorem derives its name from the fact that we’re using a parameter of central tendency – typically the mean, median, or mode– to arrive at the limiting distribution of independent random variables– which, under certain conditions, will typically be the standard normal distribution.\nTo put it differently, by sampling a finite population an increasing number of times and using the mean of the \\(n\\) samples as our metric, we will see how our histogram for the population of sampled means will conform to a standard normal curve. By estimating the limiting distribution– the beloved normal distribution– of our independent variables, the theorem helps us answer the question:\n\nIf an outcome \\(p\\) has occurred \\(p\\) times out of \\(p + q = n\\) number of trials, what’s the probability of our desired outcome occurring \\(r\\) times in a further \\(r + q = m\\) number of trials?\n\nLet’s now talk about the conditions we must meet for our distribution to converge or arrive at the standard normal distribution with mean equal to zero and variance equal to 1.\n\n\nI’ll start referring to a standard normal distribution with mean equal to zero and variance equal to 1 by using the following notation \\(\\mathcal{N}(0,1)\\)\n\nThe variables under study must be the outcome of a random phenomenon for which we know the type of outcomes that can happen but cannot know how or when they will occur. We can know some of the causes of this phenomenon, but their deviations from a supposed pattern are unknown to us.\nA classic example of a random variable is a game of cards in which we know what the cards on the deck are but cannot tell which one will come up next– unless we cheat and then the whole thing is not a random phenomenon anymore. In the life and social sciences, we can find random variables everywhere: the height and weight of a population, hemoglobin levels in healthy adults, the number of patients in the ER on any given night, pH levels in different types of topsoil or the scores on the UCLA Loneliness scale. The common thread here is that one random observation doesn’t provide information on the occurrence of the next one.\n\nIn the life and social sciences, we’ll typically see conditional independence, where variables \\(A\\) and \\(B\\) are measurements of the same underlying quantity \\(C\\). We say that \\(A\\) and \\(B\\) are conditionally independent if, once C is known, the value of \\(B\\) doesn’t give new information about \\(A\\). Knowing what happens to \\(B\\) after \\(C\\) takes place doesn’t increase our chances of correctly predicting the outcome of \\(A\\). In addition, independent variables are also uncorrelated – their covariance (\\(cov[A,B] = E[A]\\cdot E[B]\\)) and Pearson’s \\(r\\) (\\(\\rho_{X,Y} = \\frac{cov[A,B]}{\\sigma_{A}\\sigma_{B}}\\)​) are equal to zero.\n\nCodeclrs &lt;- c( # Creating custom color palette\n  \"#FFBE00\", #  yellow\n  \"#B92F0A\", #  red\n  \"#7C225C\", #  purple\n  \"#792A26\", #  brown\n  \"#242424\", #  dark gray\n  \"#394DAA\") #  blue\n\nxx &lt;- seq(-5,5,by=.01) # Creating variable to use in dens. curve\n## Plot graphics\npar(las = 1, mfrow = c(1,3), mai = c(.5, .5, .5, .1))\n## Plot graphics\nplot(xx, dnorm(xx, -1),type = \"b\", lwd = 1, xlab = \"\", ylab = \"\",\n  col = clrs[3], main = \"Identically distributed\")\nlines(xx, dnorm(xx, -1), lty = 2, lwd = 3, pch = 18, col = clrs[1])\nlegend(\"topright\",legend = c(\"A\", \"B\"),\n  col = c(clrs[3], clrs[1]), lty = 1:2, cex = 0.8)\n## Plot graphics\nplot(xx,dnorm(xx,-1),type=\"l\",lwd=2,xlab=\"\",ylab=\"\",\n     col = clrs[3],main=\"Shift in means\")\nlines(xx,dnorm(xx,1),lty = 2, lwd=3,pch = 18,col= clrs[1])\nlegend(\"topright\", legend = c(\"A\", \"B\"),\n       col = c(clrs[3], clrs[1]), lty = 1:2, cex = 0.8)\n## Plot graphics\nplot(xx,dnorm(xx,-1),type=\"l\",lwd=2,xlab=\"\",ylab=\"\",\n     col = clrs[3], main = \"Different distributions\", ylim = c(0, 0.6))\nlines(xx, dchisq(xx, 2), lwd = 2, col = clrs[1])\nlegend(\"topright\", legend = c(\"A\", \"B\"),\n       col = c(clrs[3], clrs[1]), lty = 1:2, cex = 0.8)\n\n\n\n\nSimulated identically distributed and non-identically distributed variables.\n\n\nTwo variables \\(A_{n}\\) and \\(B_{n}\\) are identically distributed when their probability distributions match exactly, and their means and variances (\\(\\mu_{A} = \\mu_{B}, \\sigma_{A}^2 = \\sigma_{B}^2\\)​​​) are equal. By now you can already see how this assumption can at best be approximated and almost impossible to demonstrate by the variables commonly studied in the life and social sciences.\nLet’s look at a scenario in R:\nLet’s say a microchip manufacturer records the weekly number of defective microchips (def_micros) for each of its 8–hour shifts to evaluate whether the number of complaints (complaints) received in a 24 hours has to do with def_micro. They also want to know if the number of complaints given def_micro is related to the number of devices (devices) bought and returned through their online retailer. We would say that the complaints and devices are conditionally independent if, each variable is clearly associated withdef_micr0, but their partial correlation coefficient is close to zero (\\(\\rho = -.021\\)).\n\nCode##---------------------------------------------------------------------------------##\n## Generating two random variables that meet the i.i.d assumption\n## --------------------------------------------------------------------------------- ##\nlibrary(here)\nlibrary(ppcor)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(showtext)\nlibrary(knitr)\n##------------------------------------------------------------------------------------- \n## Generating two conditionally independent random variables\n## ------------------------------------------------------------------------------------ ##\n## Creating function to generate random variable with defined correlation to an existing var.\ncomplement &lt;- function(y, rho, x) { # corr. coefficient\n  if (missing(x)) x &lt;- rgamma(length(y), 2, .5) # Optional: supply a default if `x` is not given\n  y.perp &lt;- residuals(lm(x ~ y))\n  rho * sd(y.perp) * y + y.perp * sd(y) * sqrt(1 - rho^2)\n}\ndef_micro &lt;- rpois(1050, 6) # lambda = n*p, p = .006 defective microchips manufactured by company in 12 hrs\ncomplaints &lt;- complement(def_micro, rho = .273) # no. complaints received on 24 hr period for devices using the microchip in question\ndevices &lt;- complement(def_micro, rho = .331) # weekly no. devices processed at online retailer (+) bought (-)returned\n##\n## Creating function to summarize list of variables\ngroup_summary &lt;- function(...) {\n  summaries &lt;- lapply(list(...), summary)\n  names(summaries) &lt;- paste0(\"Summary_\", seq_along(summaries))\n  return(summaries)\n}\n\ngroup_summary(def_micro, devices, complaints)\n## $Summary_1\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    0.00    4.00    6.00    6.09    8.00   14.00 \n## \n## $Summary_2\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    -7.2     0.8     4.6     6.0     9.8    48.9 \n## \n## $Summary_3\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    -7.2    -0.3     3.3     4.5     8.0    34.8\n\n## ------------------------------------------------------------------------------- ##\n## Creating function to find Spearman's rho among all variables\n## ------------------------------------------------------------------------------- ##\nspearman_correlation &lt;- function(...) {\n  df &lt;- data.frame(...)\n  correlation_matrix &lt;- round(cor(df, method = \"spearman\"), 3)\n  return(correlation_matrix)\n}\n\nspearman &lt;- spearman_correlation(def_micro, devices, complaints)\n##\n##              def_micro devices complaints\n## def_micro      1.000   0.378      0.297\n## devices        0.378   1.000      0.099\n## complaints     0.297   0.099      1.000\n##\n## Use ppcor package to calculate partial correlation between complaints and devices\n\npart_cor &lt;- ppcor::pcor.test(devices, complaints, def_micro, method = \"spearman\")\n##      estimate  p.value  statistic  n gp   Method\n##  -0.02075897 0.501827 -0.6718506 1050  1 spearman\n## Relationship between devices and complaints is technically non-existent. We can say they're conditionally independent.\n## -------------------------------------------------------------------------------- ##\n## Creating scatterplots to visualize relationship between variables\n## -------------------------------------------------------------------------------- ##\n# Custom ggplot theme to make nicer plots\n# Get the font at https://fonts.google.com/specimen/Fira+Sans+Condensed\nfont_add_google(\"Fira Sans\")\nshowtext_auto()\ntheme_nice &lt;- function() {\n  theme_minimal(base_family = \"Fira Sans\") +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(\n        family = \"Fira Sans Bold\",\n        face = \"plain\",\n        size = rel(1.35)\n      ),\n      plot.subtitle = element_text(\n        family = \"Fira Sans Medium\",\n        face = \"plain\",\n        size = rel(1.2)\n      ),\n      axis.title = element_text(\n        family = \"Fira Sans SemiBold\",\n        face = \"plain\",\n        size = rel(1)\n      ),\n      axis.title.x = element_text(hjust = 0.5),\n      axis.title.y = element_text(hjust = 0.5),\n      axis.text = element_text(\n        family = \"Fira Sans Light\",\n        face = \"plain\",\n        size = rel(0.8)\n      ),\n      strip.text = element_text(\n        family = \"Fira Sans\",\n        face = \"bold\",\n        size = rel(1),\n        hjust = 0\n      ),\n      strip.background = element_rect(fill = \"grey95\", color = NA)\n    )\n}\n\ntheme_nice_dist &lt;- function() {\n  theme_nice() +\n    theme(\n      panel.grid = element_blank(),\n      panel.spacing.x = unit(10, units = \"pt\"),\n      axis.ticks.x = element_line(linewidth = 0.25),\n      axis.text.y = element_blank()\n    )\n}\n\ntheme_set(theme_nice())\n# Updating defaults to chosen font\nggplot2::update_geom_defaults(\"label\",list(family = \"Fira Sans SemiBold\", fontface = \"plain\"))\nggplot2::update_geom_defaults(\"text\",list(family = \"Fira Sans\", fontface = \"plain\"))\n\n# Making df with previously created variables\ndf1 &lt;- data.frame(def_micro, devices, complaints)\n# Making plot to show correlation between devices v. complaints.\ndev_comp_plot &lt;- ggplot2::ggplot(df1, aes(devices, complaints)) +\n  geom_point(color = clrs[2]) +\n  geom_smooth(method = \"glm\", # Use either glm or gam models\n    se = FALSE,\n    color = clrs[5],\n    linewidth = 1) +\n  labs(\n    x = \"devices\",\n    y = \"complaints\") +\n  theme_nice_dist() # Apply a custom theme\n# Making plot to show correlation between devices v. microchips\ndev_micro_plot &lt;- ggplot2::ggplot(df1, aes(devices, def_micro)) +\n  geom_point(color = clrs[3]) +\n  geom_smooth(method = \"lm\",\n    se = FALSE,\n    color = clrs[5],\n    linewidth = 1) +\n  labs(\n    x = \"devices\",\n    y = \"microchips\") +\n  theme_nice_dist() # Apply a classic theme\n# Making plot to show correlation between complaints v. microchips\ncomp_micro_plot &lt;- ggplot2::ggplot(df1, aes(complaints, def_micro)) +\n  geom_point(color = clrs[6]) +\n  geom_smooth(method = \"lm\",\n    se = FALSE,\n    color = clrs[5],\n    linewidth = 1) +\n  labs(\n    x = \"complaints\",\n    y = \"microchips\") +\n  theme_nice_dist() # Apply a custom theme\n# Creating grid panel displaying all three plots vertically. \narranged_plots &lt;- gridExtra::grid.arrange(dev_comp_plot, dev_micro_plot, comp_micro_plot, nrow = 1) # I prefer the vertical plots rather than the horizontal ones\n\n\n\n\nRelationship between devices, complaints, and defective microchips\n\n\n\nA process \\(Y\\) is considered additive when the value of each observation is determined by adding a variable quantity to the value of a previous observation. The process is characterized by an initial value \\(Y(0)\\) and the increments \\(Y_{i} - Y_{h}\\) with \\(i &gt; h \\geq 0\\), where \\(Y_{i}\\) represents the value of an observation at a certain point and \\(Y_{h}\\), the value of the subsequent observation. This additive process typically consists of three components.\n\nAn initial value \\(Y_{0}\\) that serves as the starting point of the process.\nA random, slow, and gradual change component \\(Y_{1}\\) around the central tendency represents the continuous evolution of the process over time.\nA third random component \\(Y_{2}\\)​ represents sudden jumps or changes in the process, which may occur unpredictably at certain time points.\n\nTogether, these components contribute to the behavior of the \\(Y\\)​ process and its evolution across time through a combination of unexpected slow changes and sudden jumps.\nAn example of such a random process in epidemiology is the spread of an infectious disease within a population over time. Initially, there’s a small number of infected (I) individuals that spread the disease across the susceptible population (S), and as the population grows and control measures take place, the evolution of the disease fluctuates. If a new variant of this disease was introduced, a sudden spike in the number of infected individuals would also determine the evolution of the disease.\n\nHow we use CLT\n\nHow to know when empirical distribution converges to normal distribution?\n\n\n\nJacob Cohen (1991) mentioned that at least 30 observations were needed to use the critical-ratio approach used in t-tables when comparing groups. If the sample size is smaller than 30, then students would be forced to “small” sample statistics.\n\nIt wasn’t until some years later that I discovered (mind you, not invented) power analysis, one of whose fruits was the revelation that for a two-independent-group-mean comparison with n = 30 per group at the sanctified two-tailed .05 level, the probability that a medium-sized effect would be labeled as significant by the most modern methods (a t-test) was only .47. Thus, it was approximately a coin flip whether one would get a significant result, even though, in reality, the effect size was meaningful.\n\n\n\nSimply a rule of thumb that allowed students to use critical t–t-tables. These tables could only span 30 lines to fit in one page. Fisher created the\nt-table and only went up to n = 30 (Fisher’s Statistical Methods for Research Workers (1925))\n\nSample size at least 30 so that the error variance between t–t-distribution and theoretical normal distribution is .25 or less from df = 30 up to infinity.\nWhen data is severely skewed neither the t–test nor the permutation test have much power (are not robust) to correctly identify a statistically significance difference in means between too highly skewed distributions. Even if your data contained tens of thousands of observations, the t–test may not recognize a statistically significance difference in means. The distribution of the data and the number of observations need to be considered when deciding whether the t–test is meaningful and accurate, sometimes thousands of observations may be needed.\n\n\n\n\n\nThe number of observations needed for our distribution to approximate the standard normal curve will ultimately depend on our data, the question we’re seeking to answer, and the test we plan to use to arrive at our hypothesis.\nUse power analysis to determine the sample size needed to obtain statistically significant results using your desired \\(\\alpha\\), \\(\\beta\\), and effect size measure. This way, you don’t have to assume\n\n\n\n\n\n\n\n\nSimulated identically distributed and non-identically distributed variables.\nRelationship between devices, complaints, and defective microchips"
  },
  {
    "objectID": "blog/2024/03/here/index.html",
    "href": "blog/2024/03/here/index.html",
    "title": "Don’t be an absolutist. Use the here package for reproducible workflows",
    "section": "",
    "text": "Don’t be an absolutist– use relative paths. Use the here package instead of setwd() or getwd() to increase reproducibility and avoid wasting your and other people’s time."
  },
  {
    "objectID": "blog/2024/03/here/index.html#tl-dr",
    "href": "blog/2024/03/here/index.html#tl-dr",
    "title": "Don’t be an absolutist. Use the here package for reproducible workflows",
    "section": "",
    "text": "Don’t be an absolutist– use relative paths. Use the here package instead of setwd() or getwd() to increase reproducibility and avoid wasting your and other people’s time."
  },
  {
    "objectID": "blog/2024/03/here/index.html#whats-the-problem-with-setwd",
    "href": "blog/2024/03/here/index.html#whats-the-problem-with-setwd",
    "title": "Don’t be an absolutist. Use the here package for reproducible workflows",
    "section": "What’s the problem with setwd()?",
    "text": "What’s the problem with setwd()?\nSince I created this website, I’ve been coding, writing, and reading a lot more which has unequivocally led to a mountain of new files and the forging of new paths– quite literally​ :smile:. At first, I kept things pretty organized, but now it’s nearly impossible to know where I saved such_and_such.txt file without wasting at least 5 minutes of my day.\nThis is what I used to do:\n\nurl1 &lt;- \"https://somefile_online_data_source_here.com\"\ndownload.file(url1, destfile = \"./data_file_here.zip\")\nunzip(\"data_file_here.zip\", exdir = getwd())\nDat &lt;- readRDS(\"summaryDat.rds\")\nDat2 &lt;- readRDS(\"SummaryDat2.rds\")\n\nMy directory will be anywhere on my device unless I have previously specified it using setwd(), but this strategy will soon be an obstacle to saving new information in an organized and reproducible way. If, later on, I change my R scripts to a different folder the original file path won’t work anymore."
  },
  {
    "objectID": "blog/2024/03/here/index.html#here-is-the-solution",
    "href": "blog/2024/03/here/index.html#here-is-the-solution",
    "title": "Don’t be an absolutist. Use the here package for reproducible workflows",
    "section": "\nhere is the solution 📁",
    "text": "here is the solution 📁\nThe here package allows you to set up a relative path mapped onto your R project directory on every device regardless of your absolute path.\nThe here function\nSuppose my directory is located in the Project folder. The here package is going to look for the .Rproj file and establish the root directory there.\n\n# Project/\n#    |\n#    |__ data/\n#    |    |___  summaryDat.rds\n#    |    |___  summaryDat2.rds\n#    |\n#    |__ blog/\n#    |    |_____index.qmd\n#    |    |\n#    |    |__ post/\n#    |    | |______ 2024/\n#    |    |       |____ 02/\n#    |    |          |____  index.qmd\n#    |    |              |____  dat3.R\n#    |    |__ img/\n#    |      |_____  plots.png\n#    |\n#    |__ scripts/\n#      |____ ind.R\n#      |____ cond.R\n\nHere you can see my root directory and how that changes with each iteration of the here command.\n\nlibrary(here)\nhere::here()\n# [1] \"C:/Users/jpmonteagudo/Desktop/R/Project\"\n  here::here(\"blog\")\n# [1] \"C:/Users/jpmonteagudo/Desktop/R/Project/blog\"\n  here::here(\"2024\")\n# [1] \"C:/Users/jpmonteagudo/Desktop/R/Project/2024\"\n  here::here(\"02\")\n# [1] \"C:/Users/jpmonteagudo/Desktop/R/Project/02\"\n  here::here(\"post\")\n# [1] \"C:/Users/jpmonteagudo/Desktop/R/Project/post\"\n  here::here(\"scripts\")\n# [1] \"C:/Users/jpmonteagudo/Desktop/R/Project/scripts\"\n#| I'll point R to the actual document by providing the full relative path\nhere::here(\"blog\",\"post\",\"2024\",\"02\",\"dat3.R\")\n# [1] \"C:/Users/jpmonteagudo/Desktop/R/Project/blog/post/2024/02/dat3.R\"\n\nI can also go up several folders at once by using the full relative path. However, when I call the here function again, it sends me back to my root directory.\n\nhere::here()\n# [1] \"C:/Users/jpmonteagudo/Desktop/R/Project\"\n\nI would use the here function to get or write files and not just be there. If I don’t need anything from my subdirectory, then R will go back to its root, the .Rproj. For example, saving a .png file with multiple plots involves specifying the relative path using here::here().\n\n\n## Using ggplot2 to save my plots\n\nggsave(\"plots.png\",arranged_plots, \n       path = here::here(\"blog\",\"2024\",\"02\",\"post\",\"img\"),\n                width = 800,\n                height = 600,\n                units = \"px\",\n                dpi = 72)\n\n## The same can be done using base R\n\ndev.copy(png,here::here(\"blog\",\"2024\",\"02\",\"clt\",\"img\",\"plots.png\"), width = 800, height = 600)\ndev.off()\n\nThe set_here function\nIf I want to “just be somewhere” anytime I open my project, I would use another function– the set_here function. Basically, this function creates a .here file anywhere in your project so you can use this directory as your root. Here’s the description in the function’s syntax\n\nWhen here encounters such a file, it uses the directory that contains this file as root. This is useful if none of the default criteria apply. You need to restart the R session so that here() picks up the newly created file.\n\n\nhere::set_here(\"blog/2024\")\n# Created file .here in C:\\Users\\jpmonteagudo\\Desktop\\R\\Project\\blog\\2024. \n# Please start a new R session in the new project directory.\n\nNext, I start a new R session here, and RStudio will automatically set my directory to this folder. I don’t need to open the R project to reach this new directory. It will give me access to the folder’s files, and I can then set a relative path to other files.\n\n# Checking directory in new R session\nhere::here()\n# [1] \"C:/Users/jpmonteagudo/Desktop/R/Project/blog/2024\"\n\nFrom this new directory, I can reach files anywhere by using the here::here() function.\nThe confusing i_am function\nThis function has given me a headache. The here package is supposed to be a tool that facilitates collaboration and connectivity, but I just couldn’t get it to work until now.\nCall the here::i_am() function at the top of your script in the first chunk of your markdown file. It will accept a relative path and then establish the new project root there. So far, it only works when I point R to a specific file I’d like to work with. If I choose a file path that’s not in my project directory, it will just point to the original directory and throw an error. If the current directory is outside of the project where the current script is running, you’ll get an error message: Could not find associated project in working directory or any parent directory.\n\n# You're in the scripts folder working on ind.R but need to access summaryDat2.rds.\n#  Simply include the relative path to the data file at the top of your script:\nlibrary(readr)\ndata &lt;- read_csv(here::i_am(\"data/summaryDat2.rds\"))\n# From my script I'm now pointing to a folder containing my Dat2.R\n\nThe dr_here function\nThe here::dr_here() shows a default message explaining why the current directory was chosen. You probably won’t use this function often– unless you’re curious and want to understand how the package selects the root directory. However, if you used here::here(\"file_path\") and got an unexpected result, go ahead and call here::dr_here. It’ll most likely ask you to create a .here file or set your directory using the here::i_am() function.\nIn the end, the here package will make it easy to collaborate and work on your projects on any device by using the here::i_am(), here::here(), and here::set_here() functions."
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Résumé",
    "section": "",
    "text": "Download résumé"
  },
  {
    "objectID": "now/index.html",
    "href": "now/index.html",
    "title": "Nowadays",
    "section": "",
    "text": "This year will be a busy one for me:\n\nI’m doing my MPH in Epidemiology and figuring out what research opportunities are available at my university.\nAfter the COVID-19 pandemic, I’ve been working from home and spent a lot of time with my family.\nI’m working full-time as a Learning and Development Specialist and doing some freelancing and working on personal projects on the side whenever I get a chance.\nI’m deepening my understanding of statistics and programming and that occupies a good chunk of my free time.\nI have been obsessed with kettlebells for the past two and a half years. I’ve mastered a few movements but there’s still a lot of ground to cover.\nEversince we moved to our new house, I’ve been working on some house repairs.\nI’ve been reading science and religion books for the past three years until recently, when I decided to reread the classics and tell them to my girls who aren’t old enough to read those yet."
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
  },
  {
    "objectID": "research/index.html#journal-articles",
    "href": "research/index.html#journal-articles",
    "title": "Research",
    "section": "Journal articles",
    "text": "Journal articles\n\n\n\n    \n        \n            \n                Julian Bashir and Elim Garak, “On the Surgical Removal of Cardassian Cranial Implants: A case study of a Cardassian patient” Starbase Deep Nine Archives 130, no. 9 (September 2178): 1425–1439\n            \n\n            \n            \n                \n                    \n                            Cranial Implants\n                        \n                    \n                    \n                            Space Travel\n                        \n                    \n                    \n                            Surgery\n                        \n                    \n                    \n                            Mock-up\n                        \n                    \n                    \n                            Placeholder\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                These brain distortions–\n            \n                 / horse loose in a hospital.\n            \n                 / Surgery?...Proceed.\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Mendeley\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\n\nNo matching items"
  }
]